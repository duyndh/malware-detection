from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier

from sklearn.decomposition import PCA
from sklearn.metrics import classification_report
import joblib

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import auc
from sklearn.metrics import plot_roc_curve
from sklearn.model_selection import GridSearchCV
from sklearn.utils import class_weight

from utils import *
from const_container import *
from feature_extractor import *


class Trainer:

    @staticmethod
    def get_hyper_parameters(classifier_id):

        if classifier_id == ClassifierEnum.get_index(ClassifierEnum.KNN):

            n_neighbors = range(1, 21)
            weights = ["uniform", "distance"]
            metric = ["euclidean", "manhattan", "minkowski"]

            tuned_parameters = [
                dict(n_neighbors=n_neighbors, weights=weights, metric=metric)
            ]
            return KNeighborsClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.SVM):

            kernel = ["linear", "poly", "rbf", "sigmoid"]
            C = np.logspace(-3, 3, 7)
            gamma = np.logspace(-3, 3, 7)
            #class_weight = ["balanced", None]

            tuned_parameters = [
                dict(kernel=kernel, C=C, gamma=gamma)
            ]
            return SVC(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Logistic_Regression):

            #penalty = ["l1", "l2", "elasticnet", "none"]
            #solver = ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]

            C = np.logspace(-3, 3, 7)
            #class_weight = ["balanced", None]

            #max_iter = [1000]

            tuned_parameters = [
                dict(penalty=["l1"], solver=["liblinear"], C=C),
                dict(penalty=["l2"], solver=["newton-cg", "lbfgs", "liblinear"], C=C),

                # dict(penalty=["elasticnet"], solver=["saga"], C=C, class_weight=class_weight),
                # dict(penalty=["none"], solver=["newton-cg", "lbfgs", "sag", "saga"], C=C, class_weight=class_weight),
            ]
            return LogisticRegression(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Neuron_Network):

            hidden_layer_sizes = [(x,) for x in range(1, 9)]
            activation = ["identity", "logistic", "tanh", "relu"]
            # solver = ["lbfgs", "sgd", "adam"]
            # learning_rate = ["constant", "invscaling", "adaptive"]

            tuned_parameters = [
                dict(hidden_layer_sizes=hidden_layer_sizes, activation=activation)
            ]

            return MLPClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Naive_Bayes):

            var_smoothing = np.logspace(-12, -6, 7)

            tuned_parameters = [
                dict(var_smoothing=var_smoothing)
            ]

            return GaussianNB(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.QDA):

            reg_param = np.logspace(-3, 0, 30)

            tuned_parameters = [
                dict(reg_param=reg_param)
            ]

            return QuadraticDiscriminantAnalysis(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.LDA):

            tuned_parameters = [
                dict(solver=["lsqr", "eigen"], shrinkage=["auto"]),
                dict(solver=["lsqr", "eigen"], shrinkage=np.logspace(-3, 0, 10)),
                dict(solver=["svd", "lsqr", "eigen"], shrinkage=[None]),
            ]

            return LinearDiscriminantAnalysis(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Decision_Tree):

            min_samples_split = np.linspace(0.1, 0.5, 5)
            min_samples_leaf = np.linspace(0.1, 0.5, 5)
            max_features = ["auto", "sqrt", "log2", None]

            tuned_parameters = [
                dict(max_depth=[int(x) for x in np.linspace(1, 100, 3)], min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_features=max_features),
                dict(max_depth=[None], min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_features=max_features)
            ]

            return DecisionTreeClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Random_Forest):

            min_samples_split = np.linspace(0.1, 0.5, 3)

            # max_features = ["auto", "sqrt", "log2", None]
            # n_estimators = [int(x) for x in np.linspace(100, 200, 3)]
            # min_samples_leaf = [int(x) for x in np.linspace(1, 10, 3)]
            # bootstrap = [True, False]

            tuned_parameters = [
                dict(max_depth=[int(x) for x in np.linspace(1, 100, 3)], min_samples_split=min_samples_split),
                dict(max_depth=[None], min_samples_split=min_samples_split)
            ]

            return RandomForestClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Extra_Trees):

            min_samples_split = np.linspace(0.1, 0.5, 3)

            # n_estimators = [int(x) for x in np.linspace(100, 200, 3)]
            # max_features = ["auto", "sqrt", "log2", None]
            # min_samples_split = np.linspace(0.1, 1, 3)
            # min_samples_leaf = [int(x) for x in np.linspace(1, 10, 3)]
            # bootstrap = [True, False]

            tuned_parameters = [
                dict(max_depth=[int(x) for x in np.linspace(1, 100, 3)], min_samples_split=min_samples_split),
                dict(max_depth=[None], min_samples_split=min_samples_split)
            ]

            return ExtraTreesClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Ada_Boost):

            learning_rate = np.logspace(-4, 0, 5)
            algorithm = ["SAMME", "SAMME.R"]

            # n_estimators = [10, 20, 50, 80, 100]

            tuned_parameters = [
                dict(learning_rate=learning_rate, algorithm=algorithm)
            ]

            return AdaBoostClassifier(), tuned_parameters

        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Gradient_Boosting):

            loss = ["deviance", "exponential"]
            learning_rate = np.logspace(-4, 0, 5)
            n_estimators = [10, 20, 50, 80, 100]

            tuned_parameters = [
                dict(loss=loss, learning_rate=learning_rate, n_estimators=n_estimators)
            ]

            return GradientBoostingClassifier(), tuned_parameters

        else:
            return None

    @staticmethod
    def init_training(X_train, X_test, scaler, decomp):

        # scale
        if scaler is None:
            scaler = StandardScaler()

        if not X_train is None:
            X_train = scaler.fit_transform(X_train)
        if not X_test is None:
            X_test = scaler.transform(X_test)

        # reduce dimension
        if not X_train is None:
            dim = len(X_train[0])
        elif not X_test is None:
            dim = len(X_test[0])
        else:
            dim = 0
        if dim > GlobalVars.reduced_size:
            if decomp is None:
                decomp = PCA(n_components=GlobalVars.reduced_size)

            if not X_train is None:
                X_train = decomp.fit_transform(X_train)
            if not X_test is None:
                X_test = decomp.transform(X_test)
        else:
            decomp = None

        # return
        return X_train, X_test, scaler, decomp

    @staticmethod
    def get_optimized_classifier(classifier_id, X, y):

        log("hyper parameters tuning: " + ClassifierEnum.get_name(classifier_id))

        classifier, parameters = Trainer.get_hyper_parameters(classifier_id)

        classifier_cv = GridSearchCV(classifier, parameters, scoring='recall_macro', cv=GlobalVars.k_fold)
        classifier_cv.fit(X, y)

        means = classifier_cv.cv_results_['mean_test_score']
        stds = classifier_cv.cv_results_['std_test_score']
        params = classifier_cv.cv_results_['params']
        for mean, std, params in zip(means, stds, params):
            log("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

        log("best parameters: " + str(classifier_cv.best_params_))
        log("best score: " + str(classifier_cv.best_score_))

        return classifier_cv

    @staticmethod
    def train_full(classifier_ids, data, labels):

        # extract feature
        X_train, extra_data = FeatureExtractor.extract_feature(GlobalVars.feature_id, data, OPCODE_RANGE, GlobalVars.sequence_len, None)
        y_train = labels

        # init
        X_train, _, scaler, decomp = Trainer.init_training(X_train, None, None, None)

        joblib.dump(scaler, open(os.path.join(GlobalVars.output_dir, "data.scl"), "wb"))
        joblib.dump(decomp, open(os.path.join(GlobalVars.output_dir, "data.dcp"), "wb"))
        joblib.dump((extra_data, len(X_train)), open(os.path.join(GlobalVars.output_dir, "data.ext"), "wb"))

        classifier_cvs = []
        for classifier_id in classifier_ids:

            try:

                classifier_name = ClassifierEnum.get_name(classifier_id)
                log("full training: " + classifier_name)

                classifier_cv = Trainer.get_optimized_classifier(classifier_id, X_train, y_train)

                # save
                joblib.dump(classifier_cv, open(os.path.join(GlobalVars.output_dir, "{0}_{1}.mdl".format(classifier_id, classifier_name)), "wb"))

                classifier_cvs.append(classifier_cv)

            except Exception as e:
                log("ERROR: " + str(e))

        return classifier_cvs

    @staticmethod
    def plot_k_fold(classifier_id, classifier, X_trains, y_trains, X_tests, y_tests):

        log("k-fold training: " + ClassifierEnum.get_name(classifier_id))

        mean_fpr = np.linspace(0, 1, 100)
        fig, ax = plt.subplots()
        tprs = []
        aucs = []

        for i_fold in range(len(X_trains)):

            log("fold #" + str(i_fold))

            # train
            classifier.fit(X_trains[i_fold], y_trains[i_fold])

            # plot
            viz = plot_roc_curve(classifier, X_tests[i_fold], y_tests[i_fold],
                                 name='ROC fold {}'.format(i_fold), alpha=0.3, lw=1, ax=ax)
            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(viz.roc_auc)

        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_auc = float(np.std(aucs))
        ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)'
                                                     % (mean_auc, std_auc), lw=2, alpha=.8)
        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\pm$ 1 std. dev.')

        ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],
               title=ClassifierEnum.get_name(classifier_id) + "\n" + str(classifier.best_params_))
        ax.legend(loc="lower right")
        fig = plt.gcf()
        plt.show()
        plt.draw()

        fig.savefig(os.path.join(GlobalVars.output_dir, str(classifier_id) + "_" + ClassifierEnum.get_name(
            classifier_id) + ".png"), dpi=100)

        return

    @staticmethod
    def train_k_fold(classifier_ids, classifier_cvs, data, labels):

        folds = StratifiedKFold(n_splits=GlobalVars.k_fold).split(data, labels)

        X_trains = []
        y_trains = []
        X_tests = []
        y_tests = []

        log("folds feature calculating")

        for i_fold, (train_ids, test_ids) in enumerate(folds):

            log("fold #" + str(i_fold))

            # extract train feature
            X_train, extra_data = FeatureExtractor.extract_feature(
                GlobalVars.feature_id, [data[train_id] for train_id in train_ids], OPCODE_RANGE,
                GlobalVars.sequence_len,
                None
            )
            y_train = [labels[train_id] for train_id in train_ids]

            # extract test feature
            X_test, _ = FeatureExtractor.extract_feature(
                GlobalVars.feature_id, [data[test_id] for test_id in test_ids], OPCODE_RANGE, GlobalVars.sequence_len,
                (extra_data, len(train_ids))
            )
            y_test = [labels[test_id] for test_id in test_ids]

            # init
            X_train, X_test, _, _ = Trainer.init_training(X_train, X_test, None, None)

            # append
            X_trains.append(X_train)
            y_trains.append(y_train)
            X_tests.append(X_test)
            y_tests.append(y_test)

        # start
        for classifier_id, classifier_cv in zip(classifier_ids, classifier_cvs):

            log("classifier " + ClassifierEnum.get_name(classifier_id))

            try:
                Trainer.plot_k_fold(classifier_id, classifier_cv, X_trains, y_trains, X_tests, y_tests)

            except Exception as e:
                log("ERROR: " + str(e))

