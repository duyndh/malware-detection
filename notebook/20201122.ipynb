{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lIBd04BVz9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1840cc82-b2db-473a-a5c3-444cac312965"
      },
      "source": [
        "pip install pefile"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/58/acf7f35859d541985f0a6ea3c34baaefbfaee23642cf11e85fe36453ae77/pefile-2019.4.18.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▎                          | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 30kB 24.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 61kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pefile) (0.16.0)\n",
            "Building wheels for collected packages: pefile\n",
            "  Building wheel for pefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pefile: filename=pefile-2019.4.18-cp36-none-any.whl size=60823 sha256=d2d2e8a0d8b5036cc070600b77cd8205b6e6c5fcb0de93ba29e4f7d83a820730\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/a1/95/4f33011a0c013c872fe6f0f364dc463a2588120820e40a30d8\n",
            "Successfully built pefile\n",
            "Installing collected packages: pefile\n",
            "Successfully installed pefile-2019.4.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbn8nmw4V-zb",
        "outputId": "c4c77225-a6ee-49fe-83d1-5435f649bfe3"
      },
      "source": [
        "pip install capstone"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting capstone\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/85/647d512c2c2e2981d6f4c70ed41c5ec3d8d6f06cc9c9dd63348e8e6a21a3/capstone-4.0.2-py2.py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.9MB/s \n",
            "\u001b[?25hInstalling collected packages: capstone\n",
            "Successfully installed capstone-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuEWSlPtWAqB",
        "outputId": "fac305c3-a903-46d6-e361-43f9197f41c3"
      },
      "source": [
        "from opcode_dataset_generator import *\n",
        "from raw_dataset_generator import *\n",
        "from feature_extractor import *\n",
        "from data_trainer import *\n",
        "from visualizer import *\n",
        "from const_container import *\n",
        "from util_wrapper import *\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "\n",
        "def init():\n",
        "    max_int = sys.maxsize\n",
        "\n",
        "    while True:\n",
        "        # decrease the maxInt value by factor 10\n",
        "        # as long as the OverflowError occurs.\n",
        "\n",
        "        try:\n",
        "            csv.field_size_limit(max_int)\n",
        "            break\n",
        "        except OverflowError:\n",
        "            max_int = int(max_int / 10)\n",
        "\n",
        "\n",
        "def generate_raw_dataset(use_benign_dataset):\n",
        "\n",
        "    if use_benign_dataset:\n",
        "\n",
        "        for machine, raw_dataset_dir in zip(\n",
        "                [X86_MACHINE, X64_MACHINE],\n",
        "                [RAW_BENIGN_DATASET_DIR_X86, RAW_BENIGN_DATASET_DIR_X64]\n",
        "                ):\n",
        "            RawDatasetGenerator.generate(\n",
        "                BENIGN_SCAN_DIRS,\n",
        "                BENIGN_SCAN_EXTS,\n",
        "                raw_dataset_dir,\n",
        "                RAW_FILE_COUNT_LIMIT,\n",
        "                FILE_SIZE_LIMIT,\n",
        "                [machine]\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        for machine, raw_dataset_dir in zip(\n",
        "                [X86_MACHINE, X64_MACHINE],\n",
        "                [RAW_MALWARE_DATASET_DIR_X86, RAW_MALWARE_DATASET_DIR_X64]\n",
        "        ):\n",
        "            RawDatasetGenerator.generate(\n",
        "                MALWARE_SCAN_DIRS,\n",
        "                MALWARE_SCAN_EXTS,\n",
        "                raw_dataset_dir,\n",
        "                RAW_FILE_COUNT_LIMIT,\n",
        "                FILE_SIZE_LIMIT,\n",
        "                [machine]\n",
        "            )\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def convert_raw_dataset_to_opcode_dataset(use_benign_dataset):\n",
        "\n",
        "    if use_benign_dataset:\n",
        "        for machine, raw_dataset_dir, output_csv_file_path in zip(\n",
        "                                [X86_MACHINE, X64_MACHINE],\n",
        "                                [RAW_BENIGN_DATASET_DIR_X86, RAW_BENIGN_DATASET_DIR_X64],\n",
        "                                [OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X86, OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X64]\n",
        "                                ):\n",
        "            loaded_files = RawDatasetGenerator.load(\n",
        "                raw_dataset_dir,\n",
        "                OPCODE_FILE_COUNT_LIMIT,\n",
        "                FILE_SIZE_LIMIT,\n",
        "                [machine]\n",
        "            )\n",
        "            OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)\n",
        "\n",
        "    else:\n",
        "        for machine, raw_dataset_dir, output_csv_file_path in zip(\n",
        "                [X86_MACHINE, X64_MACHINE],\n",
        "                [RAW_MALWARE_DATASET_DIR_X86, RAW_MALWARE_DATASET_DIR_X64],\n",
        "                [OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86, OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X64]\n",
        "        ):\n",
        "            loaded_files = RawDatasetGenerator.load(\n",
        "                raw_dataset_dir,\n",
        "                OPCODE_FILE_COUNT_LIMIT,\n",
        "                FILE_SIZE_LIMIT,\n",
        "                [machine]\n",
        "            )\n",
        "            OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def extract_opcode_feature(opcodes, feature_method, sequence_length):\n",
        "\n",
        "    if feature_method == FeatureMethod.Raw:\n",
        "        return FeatureExtractor.extract_raw_frequency(opcodes, sequence_length)\n",
        "    elif feature_method == FeatureMethod.TF:\n",
        "        return FeatureExtractor.extract_tf(opcodes, sequence_length)\n",
        "    elif feature_method == FeatureMethod.TF_IDF:\n",
        "        return FeatureExtractor.extract_tf_idf(opcodes, sequence_length)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_dataset(feature_method, sequence_length):\n",
        "\n",
        "    if True:\n",
        "\n",
        "        print(\"Training...\")\n",
        "        print(\"feature method:\", feature_method)\n",
        "        print(\"sequence length:\", sequence_length)\n",
        "\n",
        "        print(\"Load benign dataset:\", OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X86)\n",
        "        benign_opcodes_dataset = OpcodeDatasetGenerator.load(\n",
        "            OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X86,\n",
        "            TRY_BENIGN_FILE_COUNT_LIMIT)\n",
        "        benign_op_data = extract_opcode_feature(benign_opcodes_dataset, feature_method, sequence_length)\n",
        "\n",
        "        print(\"Load malware dataset:\", OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86)\n",
        "        malware_opcodes_dataset = OpcodeDatasetGenerator.load(\n",
        "            OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86,\n",
        "            TRY_MALWARE_FILE_COUNT_LIMIT)\n",
        "        malware_op_data = extract_opcode_feature(malware_opcodes_dataset, feature_method, sequence_length)\n",
        "\n",
        "        print(\"n benign:\", len(benign_op_data))\n",
        "        print(\"n malware:\", len(malware_op_data))\n",
        "\n",
        "        data = benign_op_data + malware_op_data\n",
        "        labels = [0 for x in benign_op_data] + [1 for x in malware_op_data]\n",
        "\n",
        "        # init\n",
        "        X_train, y_train, X_test, y_test, X_train_pca, X_test_pca = DataTrainer.init_train(data, labels, True)\n",
        "\n",
        "        # train\n",
        "        cross_predicts = DataTrainer.cross_train(X_train, y_train, X_test, X_train_pca, X_test_pca)\n",
        "\n",
        "        # score\n",
        "        scores = DataTrainer.cross_score(cross_predicts, y_test)\n",
        "\n",
        "        # output\n",
        "        classifiers = DataTrainer.get_classifiers()\n",
        "        for classifier_name, score in zip(classifiers, scores):\n",
        "            print(classifier_name)\n",
        "            print(score)\n",
        "\n",
        "        return\n",
        "\n",
        "    pass\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def test():\n",
        "    print(__doc__)\n",
        "\n",
        "    # Loading the Digits dataset\n",
        "    digits = datasets.load_digits()\n",
        "\n",
        "    # To apply an classifier on this data, we need to flatten the image, to\n",
        "    # turn the data in a (samples, feature) matrix:\n",
        "    n_samples = len(digits.images)\n",
        "    X = digits.images.reshape((n_samples, -1))\n",
        "    y = digits.target\n",
        "\n",
        "    # Split the dataset in two equal parts\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "    # Set the parameters by cross-validation\n",
        "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                         'C': [1, 10, 100, 1000]},\n",
        "                        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "    scores = ['precision', 'recall']\n",
        "\n",
        "    for score in scores:\n",
        "\n",
        "        clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "        print()\n",
        "\n",
        "        # print(\"Best parameters set found on development set:\")\n",
        "        # print()\n",
        "        # print(clf.best_params_)\n",
        "        # print()\n",
        "        # print(\"Grid scores on development set:\")\n",
        "        # print()\n",
        "        # means = clf.cv_results_['mean_test_score']\n",
        "        # stds = clf.cv_results_['std_test_score']\n",
        "        # for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        #     print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "        #           % (mean, std * 2, params))\n",
        "        # print()\n",
        "\n",
        "        print(\"Detailed classification report:\")\n",
        "        print()\n",
        "        print(\"The model is trained on the full development set.\")\n",
        "        print(\"The scores are computed on the full evaluation set.\")\n",
        "        print()\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "        print()\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def visualize():\n",
        "\n",
        "    Visualizer.visualize([\n",
        "        [ \"Nearest Neighbors\", 98.27, 97.62, 99.60, 95.72 ],\n",
        "        [ \"Linear SVM\", 97.69, 96.85, 98.01, 95.72 ],\n",
        "        [ \"RBF SVM\", 74.46, 47.48, 100.00, 31.13 ],\n",
        "        [ \"Decision Tree\", 95.38, 93.73, 94.47, 93.00 ],\n",
        "        [ \"Random Forest\", 90.04, 85.29, 94.34, 77.82 ],\n",
        "        [ \"Neural Net\", 98.41, 97.84, 98.81, 96.89 ],\n",
        "        [ \"Naive Bayes\", 80.52, 67.15, 89.61, 53.70 ],\n",
        "        [ \"QDA\", 95.53, 93.71, 97.88, 89.88 ],\n",
        "        [ \"LDA\", 95.24, 93.44, 95.53, 91.44 ],\n",
        "        [ \"Linear Regression\", 97.11, 96.06, 97.21, 94.94 ],\n",
        "        [ \"Logistic Regression\", 97.69, 96.86, 97.63, 96.11 ],\n",
        "        [ \"KMeans\", 65.08, 12.32, 89.47, 6.61 ],\n",
        "        [ \"AdaBoost\", 97.98, 97.25, 98.02, 96.50 ],\n",
        "        [ \"Gradient Boosting\", 98.99, 98.64, 98.83, 98.44 ]\n",
        "    ])\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # test()\n",
        "\n",
        "    init()\n",
        "\n",
        "    if ACTION == Action.COLLECT:\n",
        "        generate_raw_dataset(COLLECT_BENIGN)\n",
        "\n",
        "    elif ACTION == Action.CONVERT:\n",
        "        convert_raw_dataset_to_opcode_dataset(COLLECT_BENIGN)\n",
        "\n",
        "    elif ACTION == Action.TRAIN:\n",
        "        train_dataset(TRAINING_FEATURE_METHOD, TRAINING_SEQUENCE_LENGTH)\n",
        "\n",
        "    elif ACTION == Action.VISUALIZE:\n",
        "        visualize()\n",
        "\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "feature method: FeatureMethod.TF_IDF\n",
            "sequence length: 1\n",
            "Load benign dataset: /content/drive/MyDrive/Colab Notebooks/BenignDataset/V1/benign_opcodes_x86.csv\n",
            "Loading opcode dataset...\n",
            "0 %\n",
            "1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 %\n",
            "11 % 12 % 13 % 14 % 15 % 16 % 17 % 18 % 19 % 20 %\n",
            "21 % 22 % 23 % 24 % 25 % 26 % 27 % 28 % 29 % 30 %\n",
            "31 % 32 % 33 % 34 % 35 % 36 % 37 % 38 % 39 % 40 %\n",
            "41 % 42 % 43 % 44 % 45 % 46 % 47 % 48 % 49 % 50 %\n",
            "51 % 52 % 53 % 54 % 55 % 56 % 57 % 58 % 59 % 60 %\n",
            "61 % 62 % 63 % 64 % 65 % 66 % 67 % 68 % 69 % 70 %\n",
            "71 % 72 % 73 % 74 % 75 % 76 % 77 % 78 % 79 % 80 %\n",
            "81 % 82 % 83 % 84 % 85 % 86 % 87 % 88 % 89 % 90 %\n",
            "91 % 92 % 93 % 94 % 95 % 96 % 97 % 98 % 99 % 100 %\n",
            "Extracting TF-IDF feature...\n",
            "Load malware dataset: /content/drive/MyDrive/Colab Notebooks/MalwareDataset/V2/malware_opcodes_x86.csv\n",
            "Loading opcode dataset...\n",
            "0 %\n",
            "1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 %\n",
            "11 % 12 % 13 % 14 % 15 % 16 % 17 % 18 % 19 % 20 %\n",
            "21 % 22 % 23 % 24 % 25 % 26 % 27 % 28 % 29 % 30 %\n",
            "31 % 32 % 33 % 34 % 35 % 36 % 37 % 38 % 39 % 40 %\n",
            "41 % 42 % 43 % 44 % 45 % 46 % 47 % 48 % 49 % 50 %\n",
            "51 % 52 % 53 % 54 % 55 % 56 % 57 % 58 % 59 % 60 %\n",
            "61 % 62 % 63 % 64 % 65 % 66 % 67 % 68 % 69 % 70 %\n",
            "71 % 72 % 73 % 74 % 75 % 76 % 77 % 78 % 79 % 80 %\n",
            "81 % 82 % 83 % 84 % 85 % 86 % 87 % 88 % 89 % 90 %\n",
            "91 % 92 % 93 % 94 % 95 % 96 % 97 % 98 % 99 % 100 %\n",
            "Extracting TF-IDF feature...\n",
            "n benign: 3266\n",
            "n malware: 2583\n",
            "Initializing...\n",
            "default feature size= 256\n",
            "reduced feature size= 64\n",
            "Cross training...\n",
            "n train: 5264\n",
            "n test: 585\n",
            "Nearest Neighbors feature size= 256\n",
            "Linear SVM feature size= 256\n",
            "RBF SVM feature size= 256\n",
            "Decision Tree feature size= 256\n",
            "Random Forest feature size= 256\n",
            "Neural Net feature size= 256\n",
            "Naive Bayes feature size= 256\n",
            "QDA feature size= 64\n",
            "LDA feature size= 64\n",
            "Linear Regression feature size= 256\n",
            "Logistic Regression feature size= 64\n",
            "KMeans feature size= 256\n",
            "AdaBoost feature size= 256\n",
            "Gradient Boosting feature size= 256\n",
            "Cross scoring...\n",
            "Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "RBF SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92       343\n",
            "           1       0.83      1.00      0.91       242\n",
            "\n",
            "    accuracy                           0.92       585\n",
            "   macro avg       0.92      0.93      0.92       585\n",
            "weighted avg       0.93      0.92      0.92       585\n",
            "\n",
            "Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.97      0.95       343\n",
            "           1       0.96      0.89      0.92       242\n",
            "\n",
            "    accuracy                           0.94       585\n",
            "   macro avg       0.94      0.93      0.94       585\n",
            "weighted avg       0.94      0.94      0.94       585\n",
            "\n",
            "Neural Net\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.99      0.93       343\n",
            "           1       0.98      0.80      0.88       242\n",
            "\n",
            "    accuracy                           0.91       585\n",
            "   macro avg       0.93      0.89      0.91       585\n",
            "weighted avg       0.92      0.91      0.91       585\n",
            "\n",
            "QDA\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       343\n",
            "           1       0.98      1.00      0.99       242\n",
            "\n",
            "    accuracy                           0.99       585\n",
            "   macro avg       0.99      0.99      0.99       585\n",
            "weighted avg       0.99      0.99      0.99       585\n",
            "\n",
            "LDA\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       343\n",
            "           1       1.00      0.99      0.99       242\n",
            "\n",
            "    accuracy                           0.99       585\n",
            "   macro avg       1.00      0.99      0.99       585\n",
            "weighted avg       0.99      0.99      0.99       585\n",
            "\n",
            "Linear Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       343\n",
            "           1       1.00      0.97      0.99       242\n",
            "\n",
            "    accuracy                           0.99       585\n",
            "   macro avg       0.99      0.99      0.99       585\n",
            "weighted avg       0.99      0.99      0.99       585\n",
            "\n",
            "Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "KMeans\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75       343\n",
            "           1       1.00      0.04      0.07       242\n",
            "\n",
            "    accuracy                           0.60       585\n",
            "   macro avg       0.80      0.52      0.41       585\n",
            "weighted avg       0.76      0.60      0.47       585\n",
            "\n",
            "AdaBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n",
            "Gradient Boosting\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       343\n",
            "           1       1.00      1.00      1.00       242\n",
            "\n",
            "    accuracy                           1.00       585\n",
            "   macro avg       1.00      1.00      1.00       585\n",
            "weighted avg       1.00      1.00      1.00       585\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}