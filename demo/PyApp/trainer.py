from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier

from sklearn.decomposition import PCA
from sklearn.metrics import classification_report
import joblib

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import auc
from sklearn.metrics import plot_roc_curve

from const_container import ClassifierEnum
from utils import *
from main import *
from const_container import *

class Trainer:

    @staticmethod
    def get_classifier(classifier_id):

        if classifier_id == ClassifierEnum.get_index(ClassifierEnum.KNN):
            return KNeighborsClassifier(3)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.RBF_SVM):
            return SVC(gamma=2, C=1)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Logistic_Regression):
            return LogisticRegression(random_state=0, max_iter=1000)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Neuron_Network):
            return MLPClassifier(alpha=1, max_iter=1000)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Naive_Bayes):
            return GaussianNB()
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.QDA):
            return QuadraticDiscriminantAnalysis()
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.LDA):
            return LinearDiscriminantAnalysis()
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Decision_Tree):
            return DecisionTreeClassifier(max_depth=5)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Random_Forest):
            return RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Extra_Trees):
            return ExtraTreesClassifier(n_estimators=100, random_state=0)
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Ada_Boost):
            return AdaBoostClassifier()
        elif classifier_id == ClassifierEnum.get_index(ClassifierEnum.Gradient_Boosting):
            return GradientBoostingClassifier(random_state=0)
        else:
            return None

    @staticmethod
    def init_training(X_train, y_train, X_test, y_test, scaler=None, decomp=None):

        # scale
        if scaler is None:
            scaler = StandardScaler()

        if not X_train is None:
            X_train = scaler.fit_transform(X_train)
        if not X_test is None:
            X_test = scaler.transform(X_test)

        # reduce dimension
        if not X_train is None:
            dim = len(X_train[0])
        elif not X_test is None:
            dim = len(X_test[0])
        else:
            dim = 0
        if dim > GlobalVars.reduced_size:
            if decomp is None:
                decomp = PCA(n_components=GlobalVars.reduced_size)

            if not X_train is None:
                X_train = decomp.fit_transform(X_train)
            if not X_test is None:
                X_test = decomp.transform(X_test)
        else:
            decomp = None

        # return
        return X_train, y_train, X_test, y_test, scaler, decomp

    @staticmethod
    def train_k_fold(classifier_id, X, y):

        log("k-fold training: " + ClassifierEnum.get_name(classifier_id))
        classifier = Trainer.get_classifier(classifier_id)

        splited = StratifiedKFold(n_splits=5).split(X, y)

        mean_fpr = np.linspace(0, 1, 100)
        fig, ax = plt.subplots()
        tprs = []
        aucs = []

        for i, (train_ids, test_ids) in enumerate(splited):

            # extract train feature
            X_train, extra_data = FeatureExtractor.extract_feature(
                GlobalVars.feature_id, [X[id] for id in train_ids], OPCODE_RANGE, GlobalVars.sequence_len,
                None
            )
            y_train = [y[id] for id in train_ids]

            # extract test feature
            X_test, _ = FeatureExtractor.extract_feature(
                GlobalVars.feature_id, [X[id] for id in test_ids], OPCODE_RANGE, GlobalVars.sequence_len,
                (extra_data, len(train_ids))
            )
            y_test = [y[id] for id in test_ids]

            # init
            X_train, y_train, X_test, y_test, _, _ = Trainer.init_training(X_train, y_train, X_test, y_test)

            # train
            classifier.fit(X_train, y_train)

            # plot
            viz = plot_roc_curve(classifier, X_test, y_test, name='ROC fold {}'.format(i), alpha=0.3, lw=1, ax=ax)
            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(viz.roc_auc)

        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_auc = np.std(aucs)
        ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)'
                                                     % (mean_auc, std_auc), lw=2, alpha=.8)

        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\pm$ 1 std. dev.')

        ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=ClassifierEnum.get_name(classifier_id))
        ax.legend(loc="lower right")
        fig = plt.gcf()
        plt.show()
        plt.draw()

        fig.savefig(GlobalVars.output_dir + "\\" + str(classifier_id) + "_" + ClassifierEnum.get_name(
            classifier_id) + ".png", dpi=100)

    @staticmethod
    def train_full(classifier_id, X, y):

        classifier_name = ClassifierEnum.get_name(classifier_id)
        log("full training: " + classifier_name)
        classifier = Trainer.get_classifier(classifier_id)

        # extract feature
        X_train, extra_data = FeatureExtractor.extract_feature(
            GlobalVars.feature_id, X, OPCODE_RANGE, GlobalVars.sequence_len,
            None)
        y_train = y

        # init
        X_train, y_train, X_test, y_test, scaler, decomp = Trainer.init_training(X_train, y_train, None, None)

        # train
        classifier.fit(X_train, y_train)

        # save
        joblib.dump(classifier, open("{0}\\{1}_{2}{3}".format(
                GlobalVars.output_dir,
                classifier_id,
                classifier_name,
                ".mdl"),
            "wb"))

        joblib.dump(scaler, open("{0}\\{1}_{2}{3}".format(
                GlobalVars.output_dir,
                classifier_id,
                classifier_name,
                ".scl"),
            "wb"))

        if not decomp is None:
            joblib.dump(decomp, open("{0}\\{1}_{2}{3}".format(
                    GlobalVars.output_dir,
                    classifier_id,
                    classifier_name,
                    ".dcp"),
                "wb"))

        if not extra_data is None:
            joblib.dump((extra_data, len(X_train)), open("{0}\\{1}_{2}{3}".format(
                    GlobalVars.output_dir,
                    classifier_id,
                    classifier_name,
                    ".ext"),
                "wb"))

    @staticmethod
    def test_model(classifier_id, X, y):

        classifier_name = ClassifierEnum.get_name(classifier_id)
        log("validate model: " + classifier_name)

        # load and transform
        classifier = joblib.load(open("{0}\\{1}_{2}{3}".format(
            GlobalVars.output_dir,
            classifier_id,
            classifier_name,
            ".mdl"),
            "rb"))

        scaler = joblib.load(open("{0}\\{1}_{2}{3}".format(
            GlobalVars.output_dir,
            classifier_id,
            classifier_name,
            ".scl"),
            "rb"))

        decomp_path = "{0}\\{1}_{2}{3}".format(
            GlobalVars.output_dir,
            classifier_id,
            classifier_name,
            ".dcp")
        if os.path.exists(decomp_path):
            decomp = joblib.load(open(decomp_path, "rb"))
        else:
            decomp = None

        extra_path = "{0}\\{1}_{2}{3}".format(
            GlobalVars.output_dir,
            classifier_id,
            classifier_name,
            ".ext")
        if os.path.exists(extra_path):
            extra_data_with_count = joblib.load(open(extra_path, "rb"))
        else:
            extra_data_with_count = None

        # extract feature
        X_test, _ = FeatureExtractor.extract_feature(
            GlobalVars.feature_id, X, OPCODE_RANGE, GlobalVars.sequence_len, extra_data_with_count
        )
        y_test = y

        # init
        _, _, X_test, y_test, scaler, decomp = Trainer.init_training(None, None, X_test, y_test, scaler, decomp)

        # predict
        raw_predicts = classifier.predict(X_test)
        y_pred = np.stack([(1 if int(round(x)) > 0 else 0) for x in raw_predicts], axis=0)

        log("\n" + classification_report(y_test, y_pred))

