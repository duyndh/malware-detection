from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier

from sklearn.decomposition import PCA
from sklearn.metrics import classification_report
import joblib

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import auc
from sklearn.metrics import plot_roc_curve

from const_container import *

class Trainer:

    @staticmethod
    def get_classifiers():
        return {
            #"K Nearest Neighbors": KNeighborsClassifier(3),
            #"Linear SVM": SVC(kernel="linear", C=0.025),
            #"RBF SVM": SVC(gamma=2, C=1),
            #"Linear Regression": LinearRegression(),
            "Logistic Regression": LogisticRegression(random_state=0, max_iter=1000),
            #"Neural Net": MLPClassifier(alpha=1, max_iter=1000),
            #"Naive Bayes": GaussianNB(),
            #"QDA": QDA(),
            #"LDA": LDA(),
            #"Decision Tree": DecisionTreeClassifier(max_depth=5),
            #"Random Forest": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            #"Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=0),
            #"AdaBoost": AdaBoostClassifier(),
            "Gradient Boosting": GradientBoostingClassifier(random_state=0)
        }

    @staticmethod
    def internal_train(classifier, X_train, y_train, X_test):

        # fit model
        classifier.fit(X_train, y_train)

        # calculate accuracy
        predicts = classifier.predict(X_test)

        return [(1 if int(round(x)) > 0 else 0) for x in predicts]

    @staticmethod
    def init_train(X, y, reduce_dimension):

        print("Initializing...")

        # create scaler
        sc = StandardScaler()

        # split to train and test set
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=42)

        # normalize
        X_train = sc.fit_transform(X_train)
        X_test = sc.transform(X_test)

        # reduce number of dimension
        #pca = PCA(n_components=REDUCED_FEATURE_SIZE)
        # X_train_pca = [[]]#pca.fit_transform(X_train)
        # X_test_pca = [[]]#pca.transform(X_test)

        # if len(X_train[0]) > MAX_FEATURE_SIZE:
        #     pca = PCA(n_components=MAX_FEATURE_SIZE)
        #     X_train = pca.fit_transform(X_train)
        #     X_test = pca.transform(X_test)

        # print("default feature size=", len(X_train[0]))
        # print("reduced feature size=", len(X_train_pca[0]))

        return X_train, y_train, X_test, y_test#, X_train_pca, X_test_pca

    # @staticmethod
    # def single_train(classifier, X_train, y_train, X_test):
    #
    #     # train
    #     predicts = Trainer.internal_train(classifier, X_train, y_train, X_test)
    #
    #     return predicts
    #
    #

    @staticmethod
    def cross_train(X_train, y_train, X_test, X_train_pca, X_test_pca, models_directory):

        print("Cross training...")

        print("n train:", len(X_train))
        print("n test:", len(X_test))

        # get classifiers
        classifiers = Trainer.get_classifiers()

        cross_predicts = []

        # iterate over classifiers
        for classifier_name in classifiers:

            print(classifier_name, end=" ")

            # train
            if classifier_name == "LDA" or classifier_name == "QDA" or classifier_name == "Logistic Regression":
                print("feature size=", len(X_train_pca[0]))
                cross_predicts.append(Trainer.internal_train(classifiers[classifier_name],
                                                                 X_train_pca, y_train, X_test_pca))
            else:
                print("feature size=", len(X_train[0]))
                cross_predicts.append(Trainer.internal_train(classifiers[classifier_name],
                                                                 X_train, y_train, X_test))

            #joblib.dump(classifiers[classifier_name], os.path.join(models_directory, classifier_name + ".xml"))

            pass

        return cross_predicts

    @staticmethod
    def train_report(classifier_name, classifier, X_train, y_train, X_test, y_test):

        y_preds = np.stack(Trainer.internal_train(classifier, X_train, y_train, X_test), axis=0)

        print(classification_report(y_test, y_preds))
        pass

    @staticmethod
    def train_k_fold(classifier_name, classifier, X, y):
        cv = StratifiedKFold(n_splits=10)
        tprs = []
        aucs = []
        mean_fpr = np.linspace(0, 1, 100)

        fig, ax = plt.subplots()
        for i, (train, test) in enumerate(cv.split(X, y)):
            X_train = X[train]
            X_test = X[test]
            y_train = y[train]
            y_test = y[test]
            classifier.fit(X_train, y_train)
            viz = plot_roc_curve(classifier, X_test, y_test, name='ROC fold {}'.format(i), alpha=0.3, lw=1, ax=ax)
            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(viz.roc_auc)

        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)

        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_auc = np.std(aucs)
        ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc), lw=2,
                alpha=.8)

        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\pm$ 1 std. dev.')

        ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=classifier_name)
        ax.legend(loc="lower right")
        plt.show()