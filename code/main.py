from opcode_dataset_generator import *
from raw_dataset_generator import *
from feature_extractor import *
from data_trainer import *
from visualizer import *
from const_container import *
from util_wrapper import *

import sys


def init():
    max_int = sys.maxsize

    while True:
        # decrease the maxInt value by factor 10
        # as long as the OverflowError occurs.

        try:
            csv.field_size_limit(max_int)
            break
        except OverflowError:
            max_int = int(max_int / 10)


def generate_raw_dataset(use_benign_dataset):

    if use_benign_dataset:

        for machine, raw_dataset_dir in zip(
                [x86_machine, x64_machine],
                [raw_benign_dataset_dir_x86, raw_benign_dataset_dir_x64]
                ):
            RawDatasetGenerator.generate(
                benign_scan_dirs,
                benign_scan_exts,
                raw_dataset_dir,
                raw_file_count_limit,
                file_size_limit,
                [machine]
            )

    else:
        for machine, raw_dataset_dir in zip(
                [x86_machine, x64_machine],
                [raw_malware_dataset_dir_x86, raw_malware_dataset_dir_x64]
        ):
            RawDatasetGenerator.generate(
                benign_scan_dirs,
                benign_scan_exts,
                raw_dataset_dir,
                raw_file_count_limit,
                file_size_limit,
                [machine]
            )

    pass


def convert_raw_dataset_to_opcode_dataset(use_benign_dataset):

    if use_benign_dataset:
        for machine, raw_dataset_dir, output_csv_file_path in zip(
                                [x86_machine, x64_machine],
                                [raw_benign_dataset_dir_x86, raw_benign_dataset_dir_x64],
                                [opcode_benign_dataset_csv_file_path_x86, opcode_benign_dataset_csv_file_path_x64]
                                ):
            loaded_files = RawDatasetGenerator.load(
                raw_dataset_dir,
                opcode_file_count_limit,
                file_size_limit,
                [machine]
            )
            OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)

    else:
        for machine, raw_dataset_dir, output_csv_file_path in zip(
                [x86_machine, x64_machine],
                [raw_malware_dataset_dir_x86, raw_malware_dataset_dir_x64],
                [opcode_malware_dataset_csv_file_path_x86, opcode_malware_dataset_csv_file_path_x64]
        ):
            loaded_files = RawDatasetGenerator.load(
                raw_dataset_dir,
                opcode_file_count_limit,
                file_size_limit,
                [machine]
            )
            OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)

    pass


def train_dataset():

    if True:
        benign_opcodes_dataset = OpcodeDatasetGenerator.load(opcode_benign_dataset_csv_file_path_x86, try_benign_file_count_limit)
        benign_op_freq = []
        for opcodes in benign_opcodes_dataset:
            benign_op_freq.append(FeatureExtractor.extract_frequency(opcodes))

        malware_opcodes_dataset = OpcodeDatasetGenerator.load(opcode_malware_dataset_csv_file_path_x86, try_malware_file_count_limit)
        malware_op_freq = []
        for opcodes in malware_opcodes_dataset:
            malware_op_freq.append(FeatureExtractor.extract_frequency(opcodes))

        print("n benign:", len(benign_op_freq))
        print("n malware:", len(malware_op_freq))

        data = benign_op_freq + malware_op_freq
        labels = [0 for x in benign_op_freq] + [1 for x in malware_op_freq]

        # init
        X_train, y_train, X_test, y_test, X_train_pca, X_test_pca = DataTrainer.init_train(data, labels, True)

        # train
        cross_predicts = DataTrainer.cross_train(X_train, y_train, X_test, X_train_pca, X_test_pca)

        # validate
        validations = DataTrainer.cross_validate(cross_predicts, y_test)

        # output
        classifiers = DataTrainer.get_classifiers()
        for classifier_name, validation in zip(classifiers, validations):
            print(classifier_name, validation)

        return

    pass

def test():

    # percent = 0;
    # percent = print_percent(1, percent, 100)
    # percent = print_percent(2, percent, 100)
    # percent = print_percent(2, percent, 100)
    # percent = print_percent(2, percent, 100)
    # percent = print_percent(2, percent, 100)
    # percent = print_percent(2, percent, 100)

    pass


def visualize():

    Visualizer.visualize([
        [ "Nearest Neighbors", 98.27, 97.62, 99.60, 95.72 ],
        [ "Linear SVM", 97.69, 96.85, 98.01, 95.72 ],
        [ "RBF SVM", 74.46, 47.48, 100.00, 31.13 ],
        [ "Decision Tree", 95.38, 93.73, 94.47, 93.00 ],
        [ "Random Forest", 90.04, 85.29, 94.34, 77.82 ],
        [ "Neural Net", 98.41, 97.84, 98.81, 96.89 ],
        [ "Naive Bayes", 80.52, 67.15, 89.61, 53.70 ],
        [ "QDA", 95.53, 93.71, 97.88, 89.88 ],
        [ "LDA", 95.24, 93.44, 95.53, 91.44 ],
        [ "Linear Regression", 97.11, 96.06, 97.21, 94.94 ],
        [ "Logistic Regression", 97.69, 96.86, 97.63, 96.11 ],
        [ "KMeans", 65.08, 12.32, 89.47, 6.61 ],
        [ "AdaBoost", 97.98, 97.25, 98.02, 96.50 ],
        [ "Gradient Boosting", 98.99, 98.64, 98.83, 98.44 ]
    ])

    pass

if __name__ == '__main__':

    test()

    init()

    if True:
        generate_raw_dataset(collect_benign)

    if False:
        convert_raw_dataset_to_opcode_dataset(collect_benign)

    if False:
        train_dataset()

    if False:
        visualize()

    pass