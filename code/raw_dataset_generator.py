import os
import glob
import csv
import pefile
import random
import shutil
import hashlib

from disassembler import *


class RawDatasetGenerator:

    @staticmethod
    def enum_file_paths(scan_dirs, csv_file_path, pe_extensions):

        print("================")
        print("Enumerating file paths...")

        file_paths = []

        percentage = 0
        print(0, "%")

        if pe_extensions is None:
            pe_extensions = [""]

        total_items_count = len(scan_dirs) * len(pe_extensions)

        item_index = 0

        for dir_index, scan_dir in enumerate(scan_dirs):
            for ext_index, extension in enumerate(pe_extensions):

                if int(item_index * 100 / total_items_count) > percentage:
                    percentage = int(item_index * 100 / total_items_count)
                    print(percentage, "%")

                find_pattern = os.path.join(scan_dir, "**", "*" + extension)
                file_paths += glob.glob(find_pattern, recursive=True)

                item_index += 1

        print("Saving...")

        with open(csv_file_path, 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=['path'])
            writer.writeheader()
            for path in file_paths:
                writer.writerow({"path": path})

        print(100, "%")
        pass

    @staticmethod
    def filter_file_paths(raw_csv_path, filtered_csv_path, file_count_limit, file_size_limit):

        print("================")
        print("Filtering file paths...")

        file_paths = []

        percentage = 0
        print(0, "%")

        with open(raw_csv_path, newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                file_paths.append(row["path"])

        file_count_limit = min(len(file_paths), file_count_limit)
        randomized_file_indexes = [x for x in range(0, len(file_paths))]
        random.shuffle(randomized_file_indexes)

        pe_files = []
        for file_index in randomized_file_indexes:

            if len(pe_files) >= file_count_limit:
                break

            if int(len(pe_files) * 100 / file_count_limit) > percentage:
                percentage = int(len(pe_files) * 100 / file_count_limit)
                print(percentage, "%")

            try:
                file_path = file_paths[file_index]

                # Exceed limit file size
                if os.stat(file_path).st_size > file_size_limit:
                    continue

                # Parse PE file
                pe = pefile.PE(file_path)

                # Only accept x64 and x86 machine type
                machine = pe.FILE_HEADER.Machine
                if machine != 0x8664 and machine != 0x14c:
                    continue

                if DisAssembler.get_code_section(pe) is None:
                    continue

                # Calculate checksum
                checksum = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
                if any(pe for pe in pe_files if pe["md5"] == checksum):
                    continue

                # Add to result
                pe_files.append({"path": file_path, "machine": machine, "md5": checksum})

            except:
                pass

        print("Saving...")

        with open(filtered_csv_path, 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["path", "machine", "md5"])
            writer.writeheader()
            for pe_file in pe_files:
                writer.writerow(pe_file)

        print(100, "%")
        pass

    @staticmethod
    def store_files(filtered_csv_path, dataset_dir, dataset_csv_name):

        print("================")
        print("Storing files...")

        src_pe_files = []
        dst_pe_files = []

        percentage = 0
        print(0, "%")

        # Create dataset dir
        if os.path.exists(dataset_dir):
            return
        else:
            os.mkdir(dataset_dir)

        with open(filtered_csv_path, newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                src_pe_files.append(row)

        for file_index, src_pe in enumerate(src_pe_files):

            if int(file_index * 100 / len(src_pe_files)) > percentage:
                percentage = int(file_index * 100 / len(src_pe_files))
                print(percentage, "%")

            src_path = src_pe["path"]
            src_name_without_ext, extension = os.path.splitext(os.path.basename(src_path))
            dst_name = src_name_without_ext + extension
            name_index = 0
            while any(pe for pe in dst_pe_files if pe["name"] == dst_name):
                name_index += 1
                dst_name = src_name_without_ext + "_" + str(name_index) + extension

            # Copy
            dst_path = os.path.join(dataset_dir, dst_name)
            shutil.copyfile(src_path, dst_path)

            # Verify checksum
            checksum = hashlib.md5(open(dst_path, 'rb').read()).hexdigest()
            if checksum != src_pe["md5"]:
                continue

            # Add to result
            dst_pe_files.append({"name": dst_name, "machine": src_pe["machine"], "md5": checksum})

        print("Saving...")

        with open(os.path.join(dataset_dir, dataset_csv_name), 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["name", "machine", "md5"])
            writer.writeheader()
            for pe_file in dst_pe_files:
                writer.writerow(pe_file)

        print(100, "%")
        pass

    @staticmethod
    def generate(src_scan_dirs, pe_extensions, dataset_dir, dataset_csv_name, file_count_limit, file_size_limit, raw_csv_path, filtered_csv_path):

        # Enum all files in scan dirs
        if True:
            RawDatasetGenerator.enum_file_paths(src_scan_dirs, raw_csv_path, pe_extensions)

        # Filter only 1000 PE files and file size <= 1MB
        if True:
            RawDatasetGenerator.filter_file_paths(raw_csv_path, filtered_csv_path, file_count_limit, file_size_limit)

        # Store into dataset
        if True:
            RawDatasetGenerator.store_files(filtered_csv_path, dataset_dir, dataset_csv_name)

        print("DONE")
        pass

    @staticmethod
    def load(dataset_dir, csv_file_name, machines_filter, file_count_limit, file_size_limit):

        print("================")
        print("Loading dataset...")

        origin_files = []

        with open(os.path.join(dataset_dir, csv_file_name), newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                origin_files.append(row)

        percentage = 0
        print(0, "%")

        loaded_files = []
        file_count_limit = min(len(origin_files), file_count_limit)

        for origin_file in origin_files:

            # Exceed limit of of file count
            if len(loaded_files) >= file_count_limit:
                break

            if int(len(loaded_files) * 100 / file_count_limit) > percentage:
                percentage = int(len(loaded_files) * 100 / file_count_limit)
                print(percentage, "%")

            # Build path
            file_path = os.path.join(dataset_dir, origin_file["name"])

            # Exceed limit of file size
            if os.stat(file_path).st_size > file_size_limit:
                continue

            # Not expected machine
            if int(row["machine"]) not in machines_filter:
                continue

            # Calculate checksum
            checksum = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
            if origin_file["md5"] != checksum:
                continue

            # Add to result
            loaded_files.append(file_path)

            pass

        print(100, "%")
        return loaded_files