import glob
import random
import shutil
import hashlib

from disassembler import *
from const_container import *
from util_wrapper import *


class RawDatasetGenerator:

    @staticmethod
    def enum_file_paths(scan_dirs, pe_extensions):

        print("================")
        print("Enumerating file paths...")

        file_paths = []

        if pe_extensions is None:
            pe_extensions = [""]

        total_items_count = len(scan_dirs) * len(pe_extensions)
        percent = 0
        print(0, "%")

        item_index = 0

        for dir_index, scan_dir in enumerate(scan_dirs):
            for ext_index, extension in enumerate(pe_extensions):

                percent = print_percent(item_index, percent, total_items_count)

                find_pattern = os.path.join(scan_dir, "**", "*" + extension)
                file_paths += glob.glob(find_pattern, recursive=True)

                item_index += 1

        print()
        print("Saving...")

        with open(TMP_RAW_FILE_PATH, 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=['path'])
            writer.writeheader()
            for path in file_paths:
                writer.writerow({"path": path})

        print(100, "%")
        pass

    @staticmethod
    def filter_file_paths(file_count, file_size, machine_filter):

        print("================")
        print("Filtering file paths...")

        file_paths = []

        percent = 0
        print(0, "%")

        with open(TMP_RAW_FILE_PATH, newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                file_paths.append(row["path"])

        file_count = min(len(file_paths), file_count)

        exceed_size_count = 0
        duplicated_count = 0
        not_match_machine_count = 0
        empty_pe_count = 0
        exception_count = 0

        checksums = {}
        pe_files = []
        for file_path in file_paths:

            if len(pe_files) >= file_count:
                break

            percent = print_percent(len(pe_files), percent, file_count)

            try:

                # Exceed limit file size
                if os.stat(file_path).st_size > file_size:
                    exceed_size_count += 1
                    continue

                # Calculate checksum
                checksum = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
                if checksum in checksums:
                    duplicated_count += 1
                    continue

                # Parse PE file
                pe = pefile.PE(file_path)

                # Only accept x64 and x86 machine type
                machine = pe.FILE_HEADER.Machine
                if machine not in machine_filter:
                    not_match_machine_count += 1
                    continue

                if DisAssembler.get_code_section(pe) is None:
                    empty_pe_count += 1
                    continue

                # Add to result
                pe_files.append({"path": file_path, "machine": machine, "md5": checksum})
                checksums[checksum] = True

            except:
                exception_count += 1
                pass

        print()
        print("exceed_size_count:", exceed_size_count)
        print("duplicated_count", duplicated_count)
        print("not_match_machine_count", not_match_machine_count)
        print("empty_pe_count", empty_pe_count)
        print("exception_count", exception_count)

        print("Saving...")

        with open(TMP_FILTERED_FILE_PATH, 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["path", "machine", "md5"])
            writer.writeheader()
            for pe_file in pe_files:
                writer.writerow(pe_file)

        print(100, "%")
        pass

    @staticmethod
    def store_files(dataset_dir):

        print("================")
        print("Storing files...")

        src_pe_files = []
        dst_pe_files = []

        percent = 0
        print(0, "%")

        # Create dataset dir
        if os.path.exists(dataset_dir):
            return
        else:
            os.mkdir(dataset_dir)

        with open(TMP_FILTERED_FILE_PATH, newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                src_pe_files.append(row)

        not_found_count = 0
        not_match_checksum_count = 0

        for file_index, src_pe in enumerate(src_pe_files):

            percent = print_percent(file_index, percent, len(src_pe_files))

            src_path = src_pe["path"]
            src_name_without_ext, extension = os.path.splitext(os.path.basename(src_path))
            dst_name = src_name_without_ext + extension
            name_index = 0
            while any(pe for pe in dst_pe_files if pe["name"] == dst_name):
                name_index += 1
                dst_name = src_name_without_ext + "_" + str(name_index) + extension

            # Copy
            dst_path = os.path.join(dataset_dir, dst_name)
            shutil.copyfile(src_path, dst_path)

            if not os.path.exists(dst_path):
                not_found_count += 1
                continue

            # Verify checksum
            checksum = hashlib.md5(open(dst_path, 'rb').read()).hexdigest()
            if checksum != src_pe["md5"]:
                not_match_checksum_count += 1
                continue

            # Add to result
            dst_pe_files.append({"name": dst_name, "machine": src_pe["machine"], "md5": checksum})

        print()
        print("not_found_count", not_found_count)
        print("not_match_checksum_count", not_match_checksum_count)

        print("Saving...")

        with open(os.path.join(dataset_dir, RAW_DATASET_CSV_FILE_NAME), 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["name", "machine", "md5"])
            writer.writeheader()
            for pe_file in dst_pe_files:
                writer.writerow(pe_file)

        print(100, "%")
        pass

    @staticmethod
    def generate(src_scan_dirs, pe_extensions, dataset_dir, file_count, file_size, machine_filter):

        # Enum all files in scan dirs
        if True:
            RawDatasetGenerator.enum_file_paths(src_scan_dirs, pe_extensions)

        # Filter only 1000 PE files and file size <= 1MB
        if True:
            RawDatasetGenerator.filter_file_paths(file_count, file_size, machine_filter)

        # Store into dataset
        if True:
            RawDatasetGenerator.store_files(dataset_dir)

        print("DONE")
        pass

    @staticmethod
    def load(dataset_dir, file_count, file_size, machines_filter):

        print("================")
        print("Loading dataset...")

        origin_files = []

        with open(os.path.join(dataset_dir, RAW_DATASET_CSV_FILE_NAME), newline='') as csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                origin_files.append(row)

        percent = 0
        print(0, "%")

        loaded_files = []
        file_count = min(len(origin_files), file_count)

        exceed_file_size_count = 0
        not_match_machine_count = 0
        not_match_checksum_count = 0

        for origin_file in origin_files:

            # Exceed limit of of file count
            if len(loaded_files) >= file_count:
                break

            percent = print_percent(len(loaded_files), percent, file_count)

            # Build path
            file_path = os.path.join(dataset_dir, origin_file["name"])

            # Exceed limit of file size
            if os.stat(file_path).st_size > file_size:
                exceed_file_size_count += 1
                continue

            # Not expected machine
            if int(origin_file["machine"]) not in machines_filter:
                not_match_machine_count += 1
                continue

            # Calculate checksum
            checksum = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
            if origin_file["md5"] != checksum:
                not_match_checksum_count += 1
                continue

            # Add to result
            loaded_files.append({"path": file_path, "machine": origin_file["machine"]})

            pass

        print()
        print("exceed_file_size_count", exceed_file_size_count)
        print("not_match_machine_count", not_match_machine_count)
        print("not_match_checksum_count", not_match_checksum_count)

        print(100, "%")
        return loaded_files

