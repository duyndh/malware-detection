from opcode_dataset_generator import *
from raw_dataset_generator import *
from feature_extractor import *
from data_trainer import *
from visualizer import *
from const_container import *
from validator import *
from util_wrapper import *
from test import *


import sys
import platform
import random
from threading import *

def init():
    max_int = sys.maxsize

    while True:
        # decrease the maxInt value by factor 10
        # as long as the OverflowError occurs.

        try:
            csv.field_size_limit(max_int)
            break
        except OverflowError:
            max_int = int(max_int / 10)


def generate_raw_dataset(use_benign_dataset):

    if use_benign_dataset:

        for machine, raw_dataset_dir in zip(
                [X86_MACHINE, X64_MACHINE],
                [RAW_BENIGN_DATASET_DIR_X86, RAW_BENIGN_DATASET_DIR_X64]
                ):
            RawDatasetGenerator.generate(
                BENIGN_SCAN_DIRS,
                BENIGN_SCAN_EXTS,
                raw_dataset_dir,
                RAW_FILE_COUNT_LIMIT,
                FILE_SIZE_LIMIT,
                [machine]
            )

    else:
        for machine, raw_dataset_dir in zip(
                [X86_MACHINE, X64_MACHINE],
                [RAW_MALWARE_DATASET_DIR_X86, RAW_MALWARE_DATASET_DIR_X64]
        ):
            RawDatasetGenerator.generate(
                MALWARE_SCAN_DIRS,
                MALWARE_SCAN_EXTS,
                raw_dataset_dir,
                RAW_FILE_COUNT_LIMIT,
                FILE_SIZE_LIMIT,
                [machine]
            )

    pass


def convert_raw_dataset_to_opcode_dataset(use_benign_dataset):

    if use_benign_dataset:
        for machine, raw_dataset_dir, output_csv_file_path in zip(
                                [X64_MACHINE],
                                [RAW_BENIGN_DATASET_DIR_X64],
                                [OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X64]
                                ):

            print(machine, raw_dataset_dir, output_csv_file_path)

            loaded_files = RawDatasetGenerator.load(
                raw_dataset_dir,
                OPCODE_FILE_COUNT_LIMIT,
                FILE_SIZE_LIMIT,
                [machine]
            )

            OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)

    else:

        if COLLECT_FROM_IDA_ASM:
            raw_dataset_dir = "c:\\Temp\\dataSample"
            loaded_files = RawDatasetGenerator.load(
                raw_dataset_dir,
                OPCODE_FILE_COUNT_LIMIT,
                FILE_SIZE_LIMIT,
                [X86_MACHINE, X64_MACHINE]
            )

            for machine, output_csv_file_path in zip(
                    [X86_MACHINE, X64_MACHINE],
                    [OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86, OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X64]
            ):
                filtered_loaded_files = [file for file in loaded_files if file["machine"] == machine]

                random.shuffle(filtered_loaded_files)
                splited_files = np.array_split(filtered_loaded_files, COLLECT_THREAD_COUNT)

                threads = []
                for thread_index in range(len(splited_files)):

                    new_thread = Thread(target=OpcodeDatasetGenerator.generate,
                                        args=(splited_files[thread_index], output_csv_file_path + ".part" + str(thread_index)))
                    new_thread.start()
                    threads.append(new_thread)

                for thread in threads:
                    thread.join()

        else:
            for machine, raw_dataset_dir, output_csv_file_path in zip(
                    [X86_MACHINE, X64_MACHINE],
                    [RAW_MALWARE_DATASET_DIR_X86, RAW_MALWARE_DATASET_DIR_X64],
                    [OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86, OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X64]
            ):
                loaded_files = RawDatasetGenerator.load(
                    raw_dataset_dir,
                    OPCODE_FILE_COUNT_LIMIT,
                    FILE_SIZE_LIMIT,
                    [machine]
                )

                OpcodeDatasetGenerator.generate(loaded_files, output_csv_file_path)

    pass


def extract_opcode_feature(opcodes, feature_method, sequence_length):

    if feature_method == FeatureMethod.Raw:
        return FeatureExtractor.extract_raw_frequency(opcodes, sequence_length)
    elif feature_method == FeatureMethod.TF:
        return FeatureExtractor.extract_tf(opcodes, sequence_length)
    elif feature_method == FeatureMethod.TF_IDF:
        return FeatureExtractor.extract_tf_idf(opcodes, sequence_length)
    else:
        return None


def train_dataset(feature_method, sequence_length):

    if True:

        print("Training...")
        print("feature method:", feature_method)
        print("sequence length:", sequence_length)

        print("Load benign dataset:", OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X86)
        benign_opcodes_dataset = OpcodeDatasetGenerator.load(
            OPCODE_BENIGN_DATASET_CSV_FILE_PATH_X86,
            TRY_BENIGN_FILE_COUNT_LIMIT)
        benign_op_data = extract_opcode_feature(benign_opcodes_dataset, feature_method, sequence_length)

        print("Load malware dataset:", OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86)
        malware_opcodes_dataset = OpcodeDatasetGenerator.load(
            OPCODE_MALWARE_DATASET_CSV_FILE_PATH_X86,
            TRY_MALWARE_FILE_COUNT_LIMIT)
        malware_op_data = extract_opcode_feature(malware_opcodes_dataset, feature_method, sequence_length)

        print("n benign:", len(benign_op_data))
        print("n malware:", len(malware_op_data))

        data = benign_op_data + malware_op_data
        labels = [0 for x in benign_op_data] + [1 for x in malware_op_data]

        # init
        X_train, y_train, X_test, y_test, X_train_pca, X_test_pca = DataTrainer.init_train(data, labels, True)

        # train
        cross_predicts = DataTrainer.cross_train(X_train, y_train, X_test, X_train_pca, X_test_pca)

        # score
        validations = Validator.cross_validate(cross_predicts, y_test)

        # output
        classifiers = DataTrainer.get_classifiers()
        for classifier_name, validation in zip(classifiers, validations):
            print(classifier_name)
            print(validation)

        return

    pass


def visualize():

    # Visualizer.visualize([
    #     [ "Nearest Neighbors", 98.27, 97.62, 99.60, 95.72 ],
    #     [ "Linear SVM", 97.69, 96.85, 98.01, 95.72 ],
    #     [ "RBF SVM", 74.46, 47.48, 100.00, 31.13 ],
    #     [ "Decision Tree", 95.38, 93.73, 94.47, 93.00 ],
    #     [ "Random Forest", 90.04, 85.29, 94.34, 77.82 ],
    #     [ "Neural Net", 98.41, 97.84, 98.81, 96.89 ],
    #     [ "Naive Bayes", 80.52, 67.15, 89.61, 53.70 ],
    #     [ "QDA", 95.53, 93.71, 97.88, 89.88 ],
    #     [ "LDA", 95.24, 93.44, 95.53, 91.44 ],
    #     [ "Linear Regression", 97.11, 96.06, 97.21, 94.94 ],
    #     [ "Logistic Regression", 97.69, 96.86, 97.63, 96.11 ],
    #     [ "KMeans", 65.08, 12.32, 89.47, 6.61 ],
    #     [ "AdaBoost", 97.98, 97.25, 98.02, 96.50 ],
    #     [ "Gradient Boosting", 98.99, 98.64, 98.83, 98.44 ]
    # ])

    pass


if __name__ == '__main__':

    init()

    # if ACTION == Action.TEST:
    #     # test()
    #     test1()
    #     # test2()
    #     pass
    #
    # if ACTION == Action.COLLECT:
    #     generate_raw_dataset(COLLECT_BENIGN)
    #
    # elif ACTION == Action.CONVERT:
    #     convert_raw_dataset_to_opcode_dataset(COLLECT_BENIGN)
    #
    # elif ACTION == Action.TRAIN:
    #     train_dataset(TRAINING_FEATURE_METHOD, TRAINING_SEQUENCE_LENGTH)
    #
    # elif ACTION == Action.VISUALIZE:
    #     visualize()

    pass