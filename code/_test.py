from pyparsing import Word, hexnums, WordEnd, Optional, alphas, alphanums
import os
import codecs
from util_wrapper import *
import base64

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,
                                     StratifiedKFold, GroupShuffleSplit,
                                     GroupKFold, StratifiedShuffleSplit)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import shutil

def test1():
    a = base64.b64decode(
        "ESCABIAQACAAACUAAAABAAAEABACwYCAACAgAAigAQFEKAAACBAgAAIIAAAAQAAAADRAQAAEAAiACAAIEABAAGgCQAThACgUAAggCgYBAgBAAAAAAAAAIAACAASAGJAA")

    print(''.join(format(x, '02x') for x in a))

    src_dir = "D:\\workspace\\Courses\\KLTN\\Datasets\\BenignDataset\\V3\\benign_dataset_x64_Data"
    dst_dir = "D:\\workspace\\Courses\\KLTN\\Datasets\\BenignDataset\\V3\\benign_dataset_x64_soft"

    checksums = {}
    names = {}
    result = []

    #percent = 0

    with open(os.path.join(dst_dir, "_dataset.csv"), newline='') as dst_csv_file:

        reader = csv.DictReader(dst_csv_file)
        data = list(reader)
        for row_index, row in enumerate(data):
            #percent = print_percent(row_index, percent, len(data))

            checksums[row["md5"]] = True
            names[row["name"]] = True
            result.append(row)

    with open(os.path.join(src_dir, "_dataset.csv"), newline='') as src_csv_file:

        reader = csv.DictReader(src_csv_file)
        data = list(reader)
        for row_index, row in enumerate(data):
            #percent = print_percent(row_index, percent, len(data))

            if row["md5"] in checksums:
                continue

            src_name_without_ext, extension = os.path.splitext(os.path.basename(row["name"]))
            dst_name = src_name_without_ext + extension
            name_index = 0
            while dst_name in names:
                name_index += 1
                dst_name = src_name_without_ext + "_" + str(name_index) + extension

            # Copy
            src_path = os.path.join(src_dir, row["name"])
            dst_path = os.path.join(dst_dir, dst_name)
            shutil.copyfile(src_path, dst_path)

            if not os.path.exists(dst_path):
                continue
            names[dst_name] = True
            result.append({"name": dst_name, "machine": row["machine"], "md5": row["md5"]})

    with open("c:\\temp\\out.csv", 'w', newline='') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=["name", "machine", "md5"])
        writer.writeheader()
        for file in result:
            writer.writerow(file)

    pass




def test():
    print(__doc__)

    # Loading the Digits dataset
    digits = datasets.load_digits()

    # To apply an classifier on this data, we need to flatten the image, to
    # turn the data in a (samples, feature) matrix:
    n_samples = len(digits.images)
    X = digits.images.reshape((n_samples, -1))
    y = digits.target

    # Split the dataset in two equal parts
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.5, random_state=0)

    # Set the parameters by cross-validation
    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                         'C': [1, 10, 100, 1000]},
                        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

    scores = ['precision', 'recall']

    for score in scores:

        clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)

        clf.fit(X_train, y_train)

        print("# Tuning hyper-parameters for %s" % score)
        print()

        # print("Best parameters set found on development set:")
        # print()
        # print(clf.best_params_)
        # print()
        # print("Grid scores on development set:")
        # print()
        # means = clf.cv_results_['mean_test_score']
        # stds = clf.cv_results_['std_test_score']
        # for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        #     print("%0.3f (+/-%0.03f) for %r"
        #           % (mean, std * 2, params))
        # print()

        print("Detailed classification report:")
        print()
        print("The model is trained on the full development set.")
        print("The scores are computed on the full evaluation set.")
        print()
        y_true, y_pred = y_test, clf.predict(X_test)
        print(classification_report(y_true, y_pred))
        print()

    pass

import csv
def test2():

    checksums = {}

    duplicated_count = 0

    with open("c:\\temp\\out.csv", 'w', newline='') as out_file:
        writer = csv.DictWriter(out_file, fieldnames=["name", "machine", "opcodes", "md5"])
        writer.writeheader()

        for file_index in range(2):
            file_path = "c:\\temp\\_dataset_" + str(file_index) + ".csv"
            print(file_path)

            with open(file_path, newline='') as in_file:
                reader = csv.DictReader(in_file)
                data = list(reader)

                for row in data:

                    if not row["md5"] in checksums:
                        writer.writerow(row)
                        checksums[row["md5"]] = True
                    else:
                        duplicated_count += 1

    pass
